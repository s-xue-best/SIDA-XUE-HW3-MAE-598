{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86fe3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.47325208459706 \n",
      " 28.824099527405245\n"
     ]
    }
   ],
   "source": [
    "T= 20; a1=8.07131  ; a11=7.43155;\n",
    "a2=1730.63 ;  a12=1554.679;\n",
    "a3=233.426;   a13=240.337;\n",
    "p_sat_w=10**(a1-a2/(T+a3))\n",
    "p_sat_d=10**(a11-a12/(T+a13))\n",
    "print(p_sat_w,'\\n',p_sat_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bbae87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array(10)\n",
    "dryness1=[0.0]\n",
    "for i in range(10):\n",
    "    dryness1.append(0.1*(i+1))\n",
    "                    \n",
    "print(dryness1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee4bdf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-22f53c2fb8f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#import torch as t\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytorch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "# A simple example of using PyTorch for gradient descent\n",
    "import numpy as np\n",
    "#import torch as t\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True) #1st is A12,2nd is A21\n",
    "p_real=[28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
    "d=[0.0]\n",
    "for i in range(10):\n",
    "    d.append(0.1*(i+1))\n",
    "                    \n",
    "\n",
    "print(d)# Define a loss\n",
    "#loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "loss=0\n",
    "for  i in  range(11):\n",
    "    p1= d[i]*exp(x[0]*(x[1]*(1-d[i])/(x[0]*d[i]+x[1]*(1-d[i])))**2)*p_sat_w\n",
    "    p2= (1-d[i])*exp(x[1]*(x[0]*d[i]/(x[0]*d[i]+x[1]*(1-d[i])))**2)*p_sat_d\n",
    "    loss=(p1+p2-p_real[i])**2+loss\n",
    "   \n",
    "\n",
    "#Take gradient\n",
    "loss.backward()\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "x.grad.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the gradient at a different x.\n",
    "x.data = t.tensor([2.0, 1.0])\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "loss.backward()\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53700b7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-21f48c6bc586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Here is a code for gradient descent without line search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Here is a code for gradient descent without line search\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Fix the step size\n",
    "a = 0.01\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(1000):  # TODO: change the termination criterion\n",
    "    loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "    loss.backward()\n",
    "    \n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        x -= a * x.grad\n",
    "        \n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        x.grad.zero_()\n",
    "        \n",
    "print(x.data.numpy())\n",
    "print(loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e79bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2333333333333334"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def objective(x):\n",
    "    \"\"\"Objective function to minimize\"\"\"\n",
    "    \n",
    "    # Create the polynomial object\n",
    "    #f = np.poly1d([1, -2, -28, 28, 12, -26, 100])\n",
    "    f=(4-2.1*x[0]**2+x[0]**4/3)*x[0]**2+x[0]*x[1]+(-4+4*x[1]**2)*x[1]**2\n",
    "    # Return the value of the polynomial\n",
    "    return f \n",
    "objective([1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space over which to evluate the function is -5 to 6\n",
    "x = np.linspace(-3, 3, 10000)\n",
    "y = objective(x)\n",
    "#x1 [-3,3]   x2 [-2,2]\n",
    "miny = min(y)\n",
    "minx = x[np.argmin(y)]\n",
    "\n",
    "# Visualize the function\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.title('Objective Function'); plt.xlabel('x'); plt.ylabel('f(x)')\n",
    "plt.vlines(minx, min(y)- 50, max(y), linestyles = '--', colors = 'r')\n",
    "plt.plot(x, y);\n",
    "\n",
    "# Print out the minimum of the function and value\n",
    "print('Minimum of %0.4f occurs at %0.4f' % (miny, minx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0840d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "# Create the domain space\n",
    "space = hp.uniform('x', -5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b54edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import rand, tpe\n",
    "\n",
    "# Create the algorithms\n",
    "tpe_algo = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7507d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "\n",
    "# Create two trials objects\n",
    "tpe_trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin\n",
    "\n",
    "# Run 2000 evals with the tpe algorithm\n",
    "tpe_best = fmin(fn=objective, space=space, algo=tpe_algo, trials=tpe_trials, \n",
    "                max_evals=2000, rstate= np.random.RandomState(50))\n",
    "\n",
    "print(tpe_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

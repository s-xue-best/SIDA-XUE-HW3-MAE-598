{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76469467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Variables wrap a Tensor\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "# Variable containing:\n",
    "# 1  1\n",
    "# 1  1\n",
    "# [torch.FloatTensor of size 2x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00e408f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2            # Create y from an operation\n",
    "# Variable containing:\n",
    "# 3  3\n",
    "# 3  3\n",
    "# [torch.FloatTensor of size 2x2]\n",
    "\n",
    "z = torch.add(x, y)  # Same as z = x + y\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bd21092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952.7181937242699\n"
     ]
    }
   ],
   "source": [
    "x=[0.2,2.0]\n",
    "p_real=[28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
    "d=[0.0]\n",
    "for i in range(10):\n",
    "    d.append(0.1*(i+1))\n",
    "                    \n",
    "#print(d)\n",
    "# Define a loss\n",
    "#loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "loss=0.0\n",
    "for  i in  range(11):\n",
    "    p1= d[i]*math.exp(x[0]*(x[1]*(1-d[i])/(x[0]*d[i]+x[1]*(1-d[i])))**2)*p_sat_w\n",
    "    p2= (1-d[i])*math.exp(x[1]*(x[0]*d[i]/(x[0]*d[i]+x[1]*(1-d[i])))**2)*p_sat_d\n",
    "    loss=(p1+p2-p_real[i])**2+loss\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 2\n",
    "\n",
    "out = z.mean()\n",
    "# Variable containing:\n",
    "# 2\n",
    "# [torch.FloatTensor of size 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b77e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def loss(x):\n",
    "    loss=0\n",
    "    length=len(x)\n",
    "    p_model=pressure(a,x)\n",
    "    for i in range(10):\n",
    "        loss=(p_model[i])\n",
    "    \n",
    "    \n",
    "def loss(a):\n",
    "    loss=0.0\n",
    "    for  i in  range(11):\n",
    "        p1= d[i]*math.exp(x[0]*(x[1]*(1-d[i])/(x[0]*d[i]+x[1]*(1-d[i])))**2)*p_sat_w\n",
    "        p2= (1-d[i])*math.exp(x[1]*(x[0]*d[i]/(x[0]*d[i]+x[1]*(1-d[i])))**2)*p_sat_d\n",
    "        loss=(p1+p2-p_real[i])**2+loss\n",
    "    print(loss)   \n",
    "    print(type(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aab5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=100\n",
    "while diff>0.1:\n",
    "    obj=loss(a,x)\n",
    "    obj.backward()\n",
    "    step=line_search(a)\n",
    "    diff=t.linalg.norm(a.grad)\n",
    "    \n",
    "    with t.no_grad\n",
    "    a=a-step*a*a.grad\n",
    "    a.grad.zero_()\n",
    "print('A12 and A21 is {}, with loss {}'.format(a.data.numpy(),obj.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple example of using PyTorch for gradient descent\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Define a loss\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "# Take gradient\n",
    "loss.backward()\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86fe3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.47325208459706 \n",
      " 28.824099527405245\n"
     ]
    }
   ],
   "source": [
    "#define the two parameter the pressure parameter\n",
    "T= 20; a1=8.07131  ; a11=7.43155;\n",
    "a2=1730.63 ;  a12=1554.679;\n",
    "a3=233.426;   a13=240.337;\n",
    "p_sat_w=10**(a1-a2/(T+a3))\n",
    "p_sat_d=10**(a11-a12/(T+a13))\n",
    "print(p_sat_w,'\\n',p_sat_d)\n",
    "# A simple example of using PyTorch for gradient descent\n",
    "import math \n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "p_real=[28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
    "x=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "#initial = t.tensor([2.0, 1.0])\n",
    "#1st is A12,2nd is A21\n",
    "\n",
    "def loss(A):   # A: is a vector x: is the ratio of one phase\n",
    "    loss=0\n",
    "    for i in range(11):\n",
    "        x1=x[i]\n",
    "        x2=1-x1\n",
    "        A12=A[0]\n",
    "        A21=A[1]\n",
    "        p_model=x1*t.exp(A12*(A21*x2/(A12*x1+A21*x2))**2)*p_sat_w+x2*t.exp(A21*(A12*x1/(A12*x1+A21*x2))**2)*p_sat_d\n",
    "        loss=loss+(p_model-p_real[i])**2\n",
    "        return loss\n",
    "def line_search(a):\n",
    "    step=0.1\n",
    "    while loss(a-step*a.grad)>(loss(a)-step*0.5*np.matmul(a.grad,a.grad)):\n",
    "        step=0.5*step\n",
    "    return step\n",
    "\n",
    "a = Variable(t.tensor([1.0, 0.1]), requires_grad=True) \n",
    "\n",
    "loss=loss(a)\n",
    "\n",
    "#Take gradient\n",
    "#loss.backward()\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "#a.grad.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1194d",
   "metadata": {},
   "source": [
    "# problem 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aee4bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.47325208459706 \n",
      " 28.824099527405245\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-d62196b497de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-d62196b497de>\u001b[0m in \u001b[0;36mline_search\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "#define the two parameter the pressure parameter\n",
    "T= 20; a1=8.07131  ; a11=7.43155;\n",
    "a2=1730.63 ;  a12=1554.679;\n",
    "a3=233.426;   a13=240.337;\n",
    "p_sat_w=10**(a1-a2/(T+a3))\n",
    "p_sat_d=10**(a11-a12/(T+a13))\n",
    "print(p_sat_w,'\\n',p_sat_d)\n",
    "# A simple example of using PyTorch for gradient descent\n",
    "import math \n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "p_real=[28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5] # pressire of measured data \n",
    "x=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "#initial = t.tensor([2.0, 1.0])\n",
    "#1st is A12,2nd is A21\n",
    "\n",
    "def loss(A):   # A: is a vector x: is the ratio of one phase\n",
    "    loss=0\n",
    "    for i in range(11):\n",
    "        x1=x[i]\n",
    "        x2=1-x1\n",
    "        A12=A[0]\n",
    "        A21=A[1]\n",
    "        p_model=x1*t.exp(A12*(A21*x2/(A12*x1+A21*x2))**2)*p_sat_w+x2*t.exp(A21*(A12*x1/(A12*x1+A21*x2))**2)*p_sat_d\n",
    "        loss=loss+(p_model-p_real[i])**2\n",
    "        return loss\n",
    "def line_search(a):\n",
    "    step=0.1\n",
    "  \n",
    "    while loss(a-step*a.grad)>loss(a)-step*0.5*np.matmul(a.grad,a.grad):\n",
    "        step=0.5*step\n",
    "    return step\n",
    "\n",
    "a = Variable(t.tensor([1.0, 1.0]), requires_grad=True) \n",
    "error=10\n",
    "while error>0.1:\n",
    "    loss=loss(a)\n",
    "    loss.backward()\n",
    "    step=line_search(a)\n",
    "    error=t.linalg.norm(a.grad)\n",
    "    \n",
    "    with t.no_grad():\n",
    "        a=a-step*a.grad\n",
    "        a.grad.zero_()\n",
    "\n",
    "print('A12 and A21 is {}, with loss {}'.format(a.data.numpy(),loss.data.numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d01ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5243201255884974\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "x=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "A=[0.1,0.2]\n",
    "def loss(A):   # A: is a vector x: is the ratio of one phase\n",
    "    loss=0\n",
    "    for i in range(11):\n",
    "        x1=x[i]\n",
    "        x2=1-x1\n",
    "        A12=A[0]\n",
    "        A21=A[1]\n",
    "        p_model=x1*math.exp(A12*(A21*x2/(A12*x1+A21*x2))**2)*p_sat_w+x2*math.exp(A21*(A12*x1/(A12*x1+A21*x2))**2)*p_sat_d\n",
    "        loss=loss+(p_model-p_real[i])**2\n",
    "        return loss\n",
    "loss=loss(A)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e0d8fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -4.], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple example of using PyTorch for gradient descent\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Define a loss\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "# Take gradient\n",
    "loss.backward()\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7447423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., -2.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine the gradient at a different x.\n",
    "x.data = t.tensor([2.0, 1.0])\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "loss.backward()\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53700b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.        1.9999971]\n",
      "8.185452e-12\n"
     ]
    }
   ],
   "source": [
    "# Here is a code for gradient descent without line search\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Fix the step size\n",
    "a = 0.01\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(1000):  # TODO: change the termination criterion\n",
    "    loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "    loss.backward()\n",
    "    \n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        x -= a * x.grad\n",
    "        \n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        x.grad.zero_()\n",
    "        \n",
    "print(x.data.numpy())\n",
    "print(loss.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b286151",
   "metadata": {},
   "source": [
    "# problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e512f414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2333333333333334"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_loss(x):\n",
    "    \"\"\"Objective function to minimize\"\"\"\n",
    "    f=(4-2.1*x[0]**2+x[0]**4/3)*x[0]**2+x[0]*x[1]+(-4+4*x[1]**2)*x[1]**2   \n",
    "    return f \n",
    "objective([1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ec29df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gp.py\n",
    "Bayesian optimisation of loss functions.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
    "    \"\"\" expected_improvement\n",
    "    Expected improvement acquisition function.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        x: array-like, shape = [n_samples, n_hyperparams]\n",
    "            The point for which the expected improvement needs to be computed.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: Numpy array.\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        n_params: int.\n",
    "            Dimension of the hyperparameter space.\n",
    "    \"\"\"\n",
    "\n",
    "    x_to_predict = x.reshape(-1, n_params)\n",
    "\n",
    "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(evaluated_loss)\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * expected_improvement\n",
    "\n",
    "\n",
    "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25):\n",
    "    \"\"\" sample_next_hyperparameter\n",
    "    Proposes the next hyperparameter to sample the loss function for.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        acquisition_func: function.\n",
    "            Acquisition function to optimise.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: array-like, shape = [n_obs,]\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        bounds: Tuple.\n",
    "            Bounds for the L-BFGS optimiser.\n",
    "        n_restarts: integer.\n",
    "            Number of times to run the minimiser with different starting points.\n",
    "    \"\"\"\n",
    "    best_x = None\n",
    "    best_acquisition_value = 1\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
    "\n",
    "        res = minimize(fun=acquisition_func,\n",
    "                       x0=starting_point.reshape(1, -1),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    return best_x\n",
    "\n",
    "\n",
    "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
    "    \"\"\" bayesian_optimisation\n",
    "    Uses Gaussian Processes to optimise the loss function `sample_loss`.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        n_iters: integer.\n",
    "            Number of iterations to run the search algorithm.\n",
    "        sample_loss: function.\n",
    "            Function to be optimised.\n",
    "        bounds: array-like, shape = [n_params, 2].\n",
    "            Lower and upper bounds on the parameters of the function `sample_loss`.\n",
    "        x0: array-like, shape = [n_pre_samples, n_params].\n",
    "            Array of initial points to sample the loss function for. If None, randomly\n",
    "            samples from the loss function.\n",
    "        n_pre_samples: integer.\n",
    "            If x0 is None, samples `n_pre_samples` initial points from the loss function.\n",
    "        gp_params: dictionary.\n",
    "            Dictionary of parameters to pass on to the underlying Gaussian Process.\n",
    "        random_search: integer.\n",
    "            Flag that indicates whether to perform random search or L-BFGS-B optimisation\n",
    "            over the acquisition function.\n",
    "        alpha: double.\n",
    "            Variance of the error term of the GP.\n",
    "        epsilon: double.\n",
    "            Precision tolerance for floats.\n",
    "    \"\"\"\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    if x0 is None:\n",
    "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "    else:\n",
    "        for params in x0:\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "    if gp_params is not None:\n",
    "        model = gp.GaussianProcessRegressor(**gp_params)\n",
    "    else:\n",
    "        kernel = gp.kernels.Matern()\n",
    "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "        if random_search:\n",
    "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "            next_sample = x_random[np.argmax(ei), :]\n",
    "        else:\n",
    "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "        cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "\n",
    "    return xp, yp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(-3, -4, 25)\n",
    "gammas = np.linspace(1, -4, 20)\n",
    "\n",
    "# We need the cartesian combination of these two vectors\n",
    "param_grid = np.array([[C, gamma] for gamma in gammas for C in lambdas])\n",
    "\n",
    "real_loss = [sample_loss(params) for params in param_grid]\n",
    "\n",
    "# The maximum is at:\n",
    "param_grid[np.array(real_loss).argmax(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "151f488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plotters.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_iteration(first_param_grid, sampled_params, sampled_loss, first_iter=0, alpha=1e-5,\n",
    "                   greater_is_better=True, true_y=None, second_param_grid=None,\n",
    "                   param_dims_to_plot=[0, 1], filepath=None, optimum=None):\n",
    "    \"\"\" plot_iteration\n",
    "    Plots a line plot (1D) or heatmap (2D) of the estimated loss function and expected\n",
    "    improvement acquisition function for each iteration of the Bayesian search algorithm.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        first_param_grid: array-like, shape = [n, 1]\n",
    "            Array containing the grid of points to plot for the first parameter.\n",
    "        sampled_params: array-like, shape = [n_points, n_params]\n",
    "            Points for which the value of the loss function is computed.\n",
    "        sampled_loss: function.\n",
    "            Values of the loss function for the parameters in `sampled_params`.\n",
    "        first_iter: int.\n",
    "            Only plot iterations after the `first_iter`-th iteration.\n",
    "        alpha: float\n",
    "            Variance of the error term in the GP model.\n",
    "        greater_is_better: boolean\n",
    "            Boolean indicating whether we want to maximise or minimise the loss function.\n",
    "        true_y: array-like, shape = [n, 1] or None\n",
    "            Array containing the true value of the loss function. If None, the real loss\n",
    "            is not plotted. (1-dimensional case)\n",
    "        second_param_grid: array-like, shape = [n, 1]\n",
    "            Array containing the grid of points to plot for the second parameter, in case\n",
    "            of a heatmap.\n",
    "        param_dims_to_plot: list of length 2\n",
    "            List containing the indices of `sampled_params` that contain the first and\n",
    "            second parameter.\n",
    "        optimum: array-like [1, n_params].\n",
    "            Maximum value of the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the GP\n",
    "    kernel = gp.kernels.Matern()\n",
    "    model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                        alpha=alpha,\n",
    "                                        n_restarts_optimizer=10,\n",
    "                                        normalize_y=True)\n",
    "\n",
    "    # Don't show the last iteration (next_sample is not available then)\n",
    "    for i in range(first_iter, sampled_params.shape[0] - 1):\n",
    "        model.fit(X=sampled_params[:(i + 1), :], y=sampled_loss[:(i + 1)])\n",
    "\n",
    "        if second_param_grid is None:\n",
    "            # 1-dimensional case: line plot\n",
    "            mu, std = model.predict(first_param_grid[:, np.newaxis], return_std=True)\n",
    "            ei = -1 * expected_improvement(first_param_grid, model, sampled_loss[:(i + 1)],\n",
    "                                           greater_is_better=greater_is_better, n_params=1)\n",
    "\n",
    "            fig, ax1, ax2 = _plot_loss_1d(first_param_grid, sampled_params[:(i + 1), :], sampled_loss[:(i + 1)], mu, std, ei, sampled_params[i + 1, :], yerr=alpha, true_y=true_y)\n",
    "        else:\n",
    "            # Transform grids into vectors for EI evaluation\n",
    "            param_grid = np.array([[first_param, second_param] for first_param in first_param_grid for second_param in second_param_grid])\n",
    "\n",
    "            mu, std = model.predict(param_grid, return_std=True)\n",
    "            ei = -1 * expected_improvement(param_grid, model, sampled_loss[:(i + 1)],\n",
    "                                           greater_is_better=greater_is_better, n_params=2)\n",
    "\n",
    "            fig, ax1, ax2 = _plot_loss_2d(first_param_grid, second_param_grid, sampled_params[:(i+1), param_dims_to_plot], sampled_loss, mu, ei, sampled_params[i + 1, param_dims_to_plot], optimum)\n",
    "\n",
    "        if file_path is not None:\n",
    "            plt.savefig('%s/bo_iteration_%d.png' % (filepath, i), bbox_inches='tight')\n",
    "\n",
    "\n",
    "def _plot_loss_1d(x_grid, x_eval, y_eval, mu, std, ei, next_sample, yerr=0.0, true_y=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8,8), sharex=True)\n",
    "\n",
    "    # Loss function plot\n",
    "    ax1.plot(x_grid, mu, label = \"GP mean\")\n",
    "    ax1.fill_between(x_grid, mu - std, mu + std, alpha=0.5)\n",
    "    ax1.errorbar(x_eval, y_eval, yerr, fmt='ok', zorder=3, label=\"Observed values\")\n",
    "    ax1.set_ylabel(\"Function value f(x)\")\n",
    "    ax1.set_xlabel(\"x\")\n",
    "\n",
    "    if true_y is not None:\n",
    "        ax1.plot(x_grid, true_y, '--', label=\"True function\")\n",
    "\n",
    "    # Acquisition function plot\n",
    "    ax2.plot(x_grid, ei, 'r', label=\"Expected improvement\")\n",
    "    ax2.set_ylabel(\"Expected improvement (EI)\")\n",
    "    ax2.set_title(\"Next sample point is C = %.3f\" % next_sample)\n",
    "    ax2.axvline(next_sample)\n",
    "\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "\n",
    "def _plot_loss_2d(first_param_grid, second_param_grid, sampled_params, sampled_loss, mu, ei, next_sample, optimum=None):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8,8), sharex=True, sharey=True)\n",
    "\n",
    "    X, Y = np.meshgrid(first_param_grid, second_param_grid, indexing='ij')\n",
    "\n",
    "    # EI contour plot\n",
    "    cp = ax1.contourf(X, Y, ei.reshape(X.shape))\n",
    "    plt.colorbar(cp, ax=ax1)\n",
    "    ax1.set_title(\"Expected Improvement. Next sample will be (%.2f, %.2f)\" % (next_sample[0], next_sample[1]))\n",
    "    ax1.autoscale(False)\n",
    "    ax1.axvline(next_sample[0], color='k')\n",
    "    ax1.axhline(next_sample[1], color='k')\n",
    "    ax1.scatter(next_sample[0], next_sample[1])\n",
    "    ax1.set_xlabel(\"C\")\n",
    "    ax1.set_ylabel(\"gamma\")\n",
    "\n",
    "    # Loss contour plot\n",
    "    cp2 = ax2.contourf(X, Y, mu.reshape(X.shape))\n",
    "    plt.colorbar(cp2, ax=ax2)\n",
    "    ax2.autoscale(False)\n",
    "    ax2.scatter(sampled_params[:, 0], sampled_params[:, 1], zorder=1)\n",
    "    ax2.axvline(next_sample[0], color='k')\n",
    "    ax2.axhline(next_sample[1], color='k')\n",
    "    ax2.scatter(next_sample[0], next_sample[1])\n",
    "    ax2.set_title(\"Mean estimate of loss surface for iteration %d\" % (sampled_params.shape[0]))\n",
    "    ax2.set_xlabel(\"C\")\n",
    "    ax2.set_ylabel(\"gamma\")\n",
    "\n",
    "    if optimum is not None:\n",
    "        ax2.scatter(optimum[0], optimum[1], marker='*', c='gold', s=150)\n",
    "\n",
    "    return fig, ax1, ax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space over which to evluate the function is -5 to 6\n",
    "x = np.linspace(-3, 3, 10000)\n",
    "y = objective(x)\n",
    "#x1 [-3,3]   x2 [-2,2]\n",
    "miny = min(y)\n",
    "minx = x[np.argmin(y)]\n",
    "\n",
    "# Visualize the function\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.title('Objective Function'); plt.xlabel('x'); plt.ylabel('f(x)')\n",
    "plt.vlines(minx, min(y)- 50, max(y), linestyles = '--', colors = 'r')\n",
    "plt.plot(x, y);\n",
    "\n",
    "# Print out the minimum of the function and value\n",
    "print('Minimum of %0.4f occurs at %0.4f' % (miny, minx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff74d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "# Create the domain space\n",
    "space = hp.uniform('x', -5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import rand, tpe\n",
    "\n",
    "# Create the algorithms\n",
    "tpe_algo = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4cfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "\n",
    "# Create two trials objects\n",
    "tpe_trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin\n",
    "\n",
    "# Run 2000 evals with the tpe algorithm\n",
    "tpe_best = fmin(fn=objective, space=space, algo=tpe_algo, trials=tpe_trials, \n",
    "                max_evals=2000, rstate= np.random.RandomState(50))\n",
    "\n",
    "print(tpe_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5581238",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81378865",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
